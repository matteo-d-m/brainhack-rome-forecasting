{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7WRFzMqfdtb"
      },
      "source": [
        "# **I Know What You Will Do: Forecasting Motor Behaviour from EEG Time Series**\n",
        "\n",
        "**[Brainhack Rome 2025](https://brainhackrome.github.io/) - Project #3**\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/matteo-d-m/brainhack-rome-forecasting/blob/main/eeg-forecasting-notebook.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMPl9cAYfdtc"
      },
      "outputs": [],
      "source": [
        "colab = True            # put False if you work locally\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if colab:\n",
        "    !pip install mne\n",
        "import mne\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "if colab:\n",
        "    from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzHeHXQsfdtc"
      },
      "source": [
        "## Load & Segment Data\n",
        "\n",
        "The data are stored as `.npy` files, so we read them into `NumPy` arrays. Their are continuous, so we must segment them into windows of interest. To begin, we choose the two seconds before event `LEDon` as window of interest."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def windows(folder, filename, sampling_rate, past_samples, future_samples):\n",
        "    \"\"\"\n",
        "    Function to extract EEG data windows based on marker events.\n",
        "\n",
        "    Parameters:\n",
        "    folder (str): Directory containing EEG files.\n",
        "    filename (str): JSON file containing marker data.\n",
        "\n",
        "    Returns:\n",
        "    all_sequences (list): List of tuples containing past and future EEG data windows.\n",
        "    \"\"\"\n",
        "\n",
        "    eeg_dir = folder\n",
        "    markers_file = filename\n",
        "\n",
        "    # Load the marker file and extract columns and data\n",
        "    with open(markers_file, 'r') as f:\n",
        "        marker_data = json.load(f)\n",
        "    columns = marker_data[\"columns\"]\n",
        "    data_rows = marker_data[\"data\"]\n",
        "\n",
        "    # Helper function to get the index of a column by name\n",
        "    def col_idx(col_name):\n",
        "        return columns.index(col_name)\n",
        "\n",
        "    all_sequences = []\n",
        "\n",
        "    # Iterate through EEG files in the specified directory\n",
        "    for eeg_filename in os.listdir(eeg_dir):\n",
        "        if eeg_filename.endswith('.npy'):  # Process only .npy files\n",
        "            eeg_path = os.path.join(eeg_dir, eeg_filename)  # Full path to the EEG file\n",
        "\n",
        "            # Use regex to extract the run number from the filename\n",
        "            m = re.search(r'_S(\\d+)', eeg_filename)\n",
        "            if m:\n",
        "                run = int(m.group(1))  # Extract run number\n",
        "            else:\n",
        "                print(f\"Could not extract run number from file name {eeg_filename}.\")\n",
        "                continue\n",
        "\n",
        "            eeg_data = np.load(eeg_path)\n",
        "\n",
        "            # Filter marker rows corresponding to the current run\n",
        "            run_rows = [row for row in data_rows if int(row[col_idx(\"Run\")]) == run]\n",
        "\n",
        "            # Process each marker row for the current run\n",
        "            for row in run_rows:\n",
        "                start_time_sec = row[col_idx(\"StartTime\")]  # Get the start time of the event\n",
        "                if start_time_sec is None:  # Skip rows with no start time\n",
        "                    continue\n",
        "\n",
        "                led_on_sec = start_time_sec\n",
        "                led_on_sample = int(led_on_sec * sampling_rate)\n",
        "\n",
        "                # Extract past and future windows around the event\n",
        "                if led_on_sample - past_samples >= 0 and led_on_sample + future_samples <= eeg_data.shape[1]:\n",
        "                    past_window = eeg_data[:, led_on_sample - past_samples : led_on_sample]\n",
        "                    future_window = eeg_data[:, led_on_sample : led_on_sample + future_samples]\n",
        "\n",
        "                    all_sequences.append((past_window, future_window))\n",
        "                else:\n",
        "                    print(f\"Skipping trial in run {run}: LEDOn sample {led_on_sample} out of bounds.\")\n",
        "\n",
        "    return all_sequences  # Return the list of (past, future) pairs"
      ],
      "metadata": {
        "id": "lmJPd1MOhdWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hPSzcVdfdtd"
      },
      "outputs": [],
      "source": [
        "if colab:\n",
        "    drive.mount('/content/drive')\n",
        "    data_dir = Path(\"/content/drive/MyDrive/Brainhack/Dataset\")\n",
        "else:\n",
        "    data_dir = Path(\"insert your local directory\")\n",
        "\n",
        "\n",
        "patients_of_interest = [1,2,3,4,5,6]\n",
        "all_sequences = []\n",
        "for patient in patients_of_interest:\n",
        "  print(f\"PATIENT P{patient}\")\n",
        "  patient_dir = data_dir / f\"P{patient}\"\n",
        "  all_patient_sequences = windows(folder=patient_dir,\n",
        "                                  filename=patient_dir / f\"P{patient}_AllLifts.json\",\n",
        "                                  sampling_rate=500,\n",
        "                                  past_samples=1000,\n",
        "                                  future_samples=1000)\n",
        "  all_sequences += all_patient_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8us31ycfdtd"
      },
      "source": [
        "## Check that all data samples have the expected shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7nRWL-Ifdtd"
      },
      "outputs": [],
      "source": [
        "NUMBER_OF_CHANNELS = 14\n",
        "NUMBER_OF_TIMEPOINTS = 1000\n",
        "\n",
        "shape_mismatches = 0\n",
        "for sample_number, sample in enumerate(all_sequences):\n",
        "  past, future = sample\n",
        "  if past.shape != future.shape:\n",
        "      print(f\"Sample {sample_number} has shape {past.shape} instead of {future.shape}\")\n",
        "      shape_mismatches += 1\n",
        "if shape_mismatches == 0:\n",
        "      print(\"Everything OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfiNpaCEfdtd"
      },
      "source": [
        "## Create PyTorch dataset and check that everything went well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PKAHBp8fdtd"
      },
      "outputs": [],
      "source": [
        "class DatasetFromList(Dataset):\n",
        "    \"\"\"Class to create a PyTorch Dataset from a list of data arrays\"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, data, normalise=False):\n",
        "        \"\"\"Class constructor (i.e., it actually creates the Dataset object)\n",
        "\n",
        "        Parameters:\n",
        "        data -- a list of data arrays (type: list[np.array])\n",
        "        normalise -- whether to normalise the samples (type: bool) (default: False)\n",
        "        \"\"\"\n",
        "\n",
        "        self.data = data\n",
        "        self.normalise = normalise\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the length of the created dataset.\n",
        "\n",
        "        Usage:\n",
        "        len(dataset_name)\n",
        "        \"\"\"\n",
        "\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Returns the data sample located at a given index\n",
        "\n",
        "        Parameters:\n",
        "        index -- the index of interest (type: int)\n",
        "\n",
        "        Returns:\n",
        "        sample -- the data sample located at 'index' (type: torch.tensor)\n",
        "\n",
        "        Usage:\n",
        "        dataset_name[index]\n",
        "        \"\"\"\n",
        "\n",
        "        sample = torch.as_tensor(self.data[index][0], dtype=torch.float32)\n",
        "        label = torch.as_tensor(self.data[index][1], dtype=torch.float32)\n",
        "        # sample = sample.unsqueeze(0)\n",
        "        if self.normalise:\n",
        "            sample = torch.nn.functional.normalize(sample)\n",
        "        return sample, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQQdMRySfdte"
      },
      "outputs": [],
      "source": [
        "dataset = DatasetFromList(data=all_sequences,\n",
        "                          normalise=False)\n",
        "\n",
        "print(f\"The PyTorch dataset contains {len(dataset)} samples, {'as expected' if len(dataset) == len(all_sequences) else 'unexpectedly'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTc1gC_afdte"
      },
      "outputs": [],
      "source": [
        "expected_shape = (NUMBER_OF_CHANNELS, NUMBER_OF_TIMEPOINTS)\n",
        "\n",
        "shape_mismatches = 0\n",
        "for sample_number, sample in enumerate(dataset):\n",
        "  past, future = sample\n",
        "  if past.shape != expected_shape and future.shape != expected_shape:\n",
        "      print(f\"Sample {sample_number} has shape {past.shape} in the past and {future.shape} in the future, instead of {expected_shape} in both\")\n",
        "if shape_mismatches == 0:\n",
        "  print(\"Everything OK\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = int(len(dataset) / 4)\n",
        "training_loader = DataLoader(dataset=dataset,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=True,\n",
        "                             num_workers=2,\n",
        "                             pin_memory=True)\n",
        "validation_loader = DataLoader(dataset=dataset,\n",
        "                               batch_size=BATCH_SIZE,\n",
        "                               shuffle=True,\n",
        "                               num_workers=2,\n",
        "                               pin_memory=True)"
      ],
      "metadata": {
        "id": "r7dx6ekm_Tg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter: WaveNet"
      ],
      "metadata": {
        "id": "4SSt2j56ubAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalConv1d(nn.Conv1d):\n",
        "    \"\"\"1D Causal convolution layer that pads inputs to avoid using future data.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, **kwargs):\n",
        "        padding = (kernel_size - 1) * dilation\n",
        "        super().__init__(in_channels, out_channels, kernel_size,\n",
        "                         padding=padding, dilation=dilation, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = super().forward(x)\n",
        "        if self.padding[0] > 0:\n",
        "            out = out[:, :, :-self.padding[0]]\n",
        "        return out\n",
        "\n",
        "class WaveNetForecaster(nn.Module):\n",
        "    def __init__(self, in_channels=14, residual_channels=32, skip_channels=64,\n",
        "                 kernel_size=2, num_layers=8):\n",
        "        \"\"\"\n",
        "        WaveNet-based forecaster model.\n",
        "        Args:\n",
        "            in_channels: Number of input channels.\n",
        "            residual_channels: Number of channels in the residual layers.\n",
        "            skip_channels: Number of channels in the skip connections.\n",
        "            kernel_size: Size of the convolutional kernel.\n",
        "            num_layers: Number of dilated causal convolution layers.\n",
        "        \"\"\"\n",
        "        super(WaveNetForecaster, self).__init__()\n",
        "        self.residual_channels = residual_channels\n",
        "        self.skip_channels = skip_channels\n",
        "\n",
        "        self.input_conv = nn.Conv1d(in_channels, residual_channels, kernel_size=1)\n",
        "\n",
        "        # lists to hold the layers for each dilated block\n",
        "        self.filter_convs = nn.ModuleList()\n",
        "        self.gate_convs = nn.ModuleList()\n",
        "        self.residual_convs = nn.ModuleList()\n",
        "        self.skip_convs = nn.ModuleList()\n",
        "\n",
        "        # exponentially increasing dilation rates\n",
        "        for i in range(num_layers):\n",
        "            dilation = 2 ** i\n",
        "            self.filter_convs.append(CausalConv1d(residual_channels, residual_channels,\n",
        "                                                  kernel_size, dilation=dilation))\n",
        "            self.gate_convs.append(CausalConv1d(residual_channels, residual_channels,\n",
        "                                                kernel_size, dilation=dilation))\n",
        "\n",
        "            self.residual_convs.append(nn.Conv1d(residual_channels, residual_channels, kernel_size=1))\n",
        "            self.skip_convs.append(nn.Conv1d(residual_channels, skip_channels, kernel_size=1))\n",
        "\n",
        "        self.output_conv1 = nn.Conv1d(skip_channels, skip_channels, kernel_size=1)\n",
        "        self.output_conv2 = nn.Conv1d(skip_channels, in_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the WaveNet model.\n",
        "        Args:\n",
        "            x: Input tensor of shape [batch, in_channels, input_length].\n",
        "        Returns:\n",
        "            Output tensor of shape [batch, in_channels, input_length].\n",
        "        \"\"\"\n",
        "        x = self.input_conv(x)\n",
        "        skip_sum = None\n",
        "\n",
        "        for filter_conv, gate_conv, res_conv, skip_conv in zip(\n",
        "                self.filter_convs, self.gate_convs, self.residual_convs, self.skip_convs):\n",
        "\n",
        "            filt = torch.tanh(filter_conv(x))\n",
        "            gate = torch.sigmoid(gate_conv(x))\n",
        "            out = filt * gate\n",
        "\n",
        "            skip_out = skip_conv(out)\n",
        "            skip_sum = skip_out if skip_sum is None else (skip_sum + skip_out)\n",
        "\n",
        "            x = res_conv(out) + x\n",
        "\n",
        "        out = torch.relu(skip_sum)\n",
        "        out = torch.relu(self.output_conv1(out))\n",
        "        out = self.output_conv2(out)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "vAWj5Bh7sjqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "def train_and_validate(model, device, combination, epochs, dataloaders):\n",
        "  \"\"\"Performs model training and validation.\n",
        "\n",
        "  Parameters:\n",
        "  model -- a PyTorch model instance\n",
        "  device -- where to run computations (torch device object)\n",
        "  combination -- a combination of hyperparameter values (namedtuple)\n",
        "  epochs -- number of model runs (int)\n",
        "  dataloaders -- PyTorch dataloader instances (tuple)\n",
        "  \"\"\"\n",
        "\n",
        "  some_loss = nn.MSELoss()\n",
        "  optimizer = optim.Adam(model.parameters(),\n",
        "                         lr=1e-2,                   # optimize\n",
        "                         weight_decay=1e-5)         # optimize\n",
        "  training_loss_log = []\n",
        "  validation_loss_log = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    training_loss = []\n",
        "    model.train()\n",
        "\n",
        "    for batch in dataloaders[0]:\n",
        "      past = batch[0].to(device)\n",
        "      true_future = batch[1].to(device)\n",
        "      generated_future = model(past)\n",
        "      loss = some_loss(generated_future, true_future)\n",
        "      model.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      loss = loss.detach().cpu().numpy()\n",
        "      training_loss.append(loss)\n",
        "    validation_loss = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for batch in dataloaders[1]:\n",
        "        past = batch[0].to(device)\n",
        "        true_future = batch[1].to(device)\n",
        "        generated_future = model(past)\n",
        "        loss = some_loss(generated_future, true_future)\n",
        "        loss = loss.detach().cpu().numpy()\n",
        "        validation_loss.append(loss)\n",
        "    training_loss = np.mean(training_loss)\n",
        "    training_loss_log.append(training_loss)\n",
        "    validation_loss = np.mean(validation_loss)\n",
        "    validation_loss_log.append(validation_loss)\n",
        "    print(f\"EPOCH {epoch+1} - TRAINING LOSS: {training_loss: .2f} - VALIDATION LOSS: {validation_loss: .2f}\")\n",
        "    if epoch == epochs-1:\n",
        "      print(\"Finished\")\n",
        "  torch.save(model.state_dict(), 'model_parameters.torch')\n",
        "  return training_loss_log, validation_loss_log"
      ],
      "metadata": {
        "id": "pFaqPy6qA2ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available else torch.device(\"CPU\")\n",
        "print(f\"Device is: {device}\")"
      ],
      "metadata": {
        "id": "LMNqfF83HhvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "model = WaveNetForecaster()\n",
        "model.to(device)\n",
        "\n",
        "losses = train_and_validate(model=model,\n",
        "                            device=device,\n",
        "                            combination=None,\n",
        "                            epochs=500,\n",
        "                            dataloaders=(training_loader, validation_loader))"
      ],
      "metadata": {
        "id": "ZIXWaFN6Hoob"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
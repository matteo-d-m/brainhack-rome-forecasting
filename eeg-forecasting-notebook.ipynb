{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7WRFzMqfdtb"
      },
      "source": [
        "# **I Know What You Will Do: Forecasting Motor Behaviour from EEG Time Series**\n",
        "\n",
        "**[Brainhack Rome 2025](https://brainhackrome.github.io/) - Project #3**\n",
        "\n",
        "Matteo De Matola<sup>1</sup><sup>°</sup>, Anna Notaro<sup>2</sup><sup>°</sup>, Emanuele Di Giorgio<sup>3</sup>, Matteo Mancini<sup>4</sup>\n",
        "\n",
        "<sup>1</sup> Center for Mind/Brain Sciences, University of Trento\n",
        "\n",
        "<sup>2</sup> Bocconi University Milan, Bocconi AI & Neuroscience Student Association\n",
        "\n",
        "<sup>3</sup> LUMSA University Rome\n",
        "\n",
        "<sup>4</sup> Centro Ricerche Enrico Fermi (CREF) Rome\n",
        "\n",
        "<sup>°</sup> equal contributions\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/matteo-d-m/brainhack-rome-forecasting/blob/main/eeg-forecasting-notebook.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMPl9cAYfdtc"
      },
      "outputs": [],
      "source": [
        "colab = True            # put False if you work locally\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from itertools import product\n",
        "from collections import namedtuple\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "if colab:\n",
        "    from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzHeHXQsfdtc"
      },
      "source": [
        "## Load & Segment Data\n",
        "\n",
        "The data are stored as `.npy` files, so we read them into `NumPy` arrays. Their are continuous, so we must segment them into windows of interest, which are centered on event `LEDon`. The resulting epochs are further segmented into a _past_ window (the one second before `LEDon`) and a _future_ window (the one second after `LEDon`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmJPd1MOhdWO"
      },
      "outputs": [],
      "source": [
        "def windows(folder, filename, sampling_rate, past_samples, future_samples, quantize=False):\n",
        "    \"\"\"\n",
        "    Function to extract EEG data windows based on marker events.\n",
        "\n",
        "    Parameters:\n",
        "    folder (str): Directory containing EEG files.\n",
        "    filename (str): JSON file containing marker data.\n",
        "\n",
        "    Returns:\n",
        "    all_sequences (list): List of tuples containing past and future EEG data windows.\n",
        "    \"\"\"\n",
        "\n",
        "    eeg_dir = folder\n",
        "    markers_file = filename\n",
        "\n",
        "    # Load the marker file and extract columns and data\n",
        "    with open(markers_file, 'r') as f:\n",
        "        marker_data = json.load(f)\n",
        "    columns = marker_data[\"columns\"]\n",
        "    data_rows = marker_data[\"data\"]\n",
        "\n",
        "    # Helper function to get the index of a column by name\n",
        "    def col_idx(col_name):\n",
        "        return columns.index(col_name)\n",
        "\n",
        "    all_sequences = []\n",
        "\n",
        "    # Iterate through EEG files in the specified directory\n",
        "    for eeg_filename in os.listdir(eeg_dir):\n",
        "        if eeg_filename.endswith('.npy'):  # Process only .npy files\n",
        "            eeg_path = os.path.join(eeg_dir, eeg_filename)  # Full path to the EEG file\n",
        "\n",
        "            # Use regex to extract the run number from the filename\n",
        "            m = re.search(r'_S(\\d+)', eeg_filename)\n",
        "            if m:\n",
        "                run = int(m.group(1))  # Extract run number\n",
        "            else:\n",
        "                print(f\"Could not extract run number from file name {eeg_filename}.\")\n",
        "                continue\n",
        "\n",
        "            eeg_data = np.load(eeg_path)\n",
        "\n",
        "            # Filter marker rows corresponding to the current run\n",
        "            run_rows = [row for row in data_rows if int(row[col_idx(\"Run\")]) == run]\n",
        "\n",
        "            # Process each marker row for the current run\n",
        "            for row in run_rows:\n",
        "                start_time_sec = row[col_idx(\"StartTime\")]  # Get the start time of the event\n",
        "                if start_time_sec is None:  # Skip rows with no start time\n",
        "                    continue\n",
        "\n",
        "                led_on_sec = start_time_sec\n",
        "                led_on_sample = int(led_on_sec * sampling_rate)\n",
        "\n",
        "                # Extract past and future windows around the event. Quantize (i.e., compress in [-1,1] if necessary)\n",
        "                if led_on_sample - past_samples >= 0 and led_on_sample + future_samples <= eeg_data.shape[1]:\n",
        "                    past_window = eeg_data[:, led_on_sample - past_samples : led_on_sample]\n",
        "                    future_window = eeg_data[:, led_on_sample : led_on_sample + future_samples]\n",
        "                    if quantize:\n",
        "                      mu = 255\n",
        "                      past_window = np.sign(past_window) * np.log(1 + mu * np.abs(past_window)) / np.log(mu + 1)\n",
        "                      future_window = np.sign(future_window) * np.log(1 + mu * np.abs(future_window)) / np.log(mu + 1)\n",
        "                    all_sequences.append((past_window, future_window))\n",
        "                else:\n",
        "                    print(f\"Skipping trial in run {run}: LEDOn sample {led_on_sample} out of bounds.\")\n",
        "\n",
        "    return all_sequences  # Return the list of (past, future) pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA8BvBajT925"
      },
      "outputs": [],
      "source": [
        "if colab:\n",
        "    drive.mount('/content/drive')\n",
        "    data_dir = Path(\"/content/drive/MyDrive/Brainhack/Dataset\")\n",
        "else:\n",
        "    data_dir = Path(\"insert your local directory\")\n",
        "\n",
        "\n",
        "\"\"\" We use 8 patients for training, two for validation and two for testing \"\"\"\n",
        "training_patients = [1,2,3,4,5,6,7,8]\n",
        "validation_patients = [9,10]\n",
        "test_patients = [11,12]\n",
        "\n",
        "datasets = {\"training\": None,\n",
        "            \"validation\": None,\n",
        "            \"test\": None}\n",
        "\n",
        "for dataset_type, patients_list in zip(datasets.keys(), [training_patients, validation_patients, test_patients]):\n",
        "  all_data = []\n",
        "  for patient in patients_list:\n",
        "    print(f\"PATIENT P{patient}\")\n",
        "    patient_dir = data_dir / f\"P{patient}\"\n",
        "    all_patient_data = windows(folder=patient_dir,\n",
        "                               filename=patient_dir / f\"P{patient}_AllLifts.json\",\n",
        "                               sampling_rate=500,\n",
        "                               past_samples=1000,\n",
        "                               future_samples=1000,\n",
        "                               quantize=True)\n",
        "    all_data += all_patient_data\n",
        "  datasets[dataset_type] = all_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8us31ycfdtd"
      },
      "source": [
        "## Check that all data samples have the expected shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7nRWL-Ifdtd"
      },
      "outputs": [],
      "source": [
        "NUMBER_OF_CHANNELS = 14\n",
        "NUMBER_OF_TIMEPOINTS = 1000\n",
        "\n",
        "expected_shape = (NUMBER_OF_CHANNELS, NUMBER_OF_TIMEPOINTS)\n",
        "\n",
        "for dataset in datasets.values():\n",
        "  shape_mismatches = 0\n",
        "  for sample_number, sample in enumerate(dataset):\n",
        "    past, future = sample\n",
        "    if past.shape != expected_shape and future.shape != expected_shape:\n",
        "        print(f\"Sample {sample_number} has shape {past.shape} in the past and {future.shape} in the future, instead of {expected_shape} in both\")\n",
        "  if shape_mismatches == 0:\n",
        "    print(\"Everything OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfiNpaCEfdtd"
      },
      "source": [
        "## Create PyTorch datasets and check that everything went well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PKAHBp8fdtd"
      },
      "outputs": [],
      "source": [
        "class DatasetFromList(Dataset):\n",
        "    \"\"\"Class to create a PyTorch Dataset from a list of data arrays\"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, data, normalise=False):\n",
        "        \"\"\"Class constructor (i.e., it actually creates the Dataset object)\n",
        "\n",
        "        Parameters:\n",
        "        data -- a list of data arrays (type: list[np.array])\n",
        "        normalise -- whether to normalise the samples (type: bool) (default: False)\n",
        "        \"\"\"\n",
        "\n",
        "        self.data = data\n",
        "        self.normalise = normalise\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the length of the created dataset.\n",
        "\n",
        "        Usage:\n",
        "        len(dataset_name)\n",
        "        \"\"\"\n",
        "\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Returns the data sample located at a given index\n",
        "\n",
        "        Parameters:\n",
        "        index -- the index of interest (type: int)\n",
        "\n",
        "        Returns:\n",
        "        sample -- the data sample located at 'index' (type: torch.tensor)\n",
        "\n",
        "        Usage:\n",
        "        dataset_name[index]\n",
        "        \"\"\"\n",
        "\n",
        "        sample = torch.as_tensor(self.data[index][0], dtype=torch.float32)\n",
        "        label = torch.as_tensor(self.data[index][1], dtype=torch.float32)\n",
        "        if self.normalise:\n",
        "            sample = torch.nn.functional.normalize(sample)\n",
        "        return sample, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQQdMRySfdte"
      },
      "outputs": [],
      "source": [
        "training_dataset = DatasetFromList(data=datasets[\"training\"],\n",
        "                                   normalise=False)\n",
        "validation_dataset = DatasetFromList(data=datasets[\"validation\"],\n",
        "                                     normalise=False)\n",
        "test_dataset = DatasetFromList(data=datasets[\"test\"],\n",
        "                               normalise=False)\n",
        "\n",
        "for dataset_type, dataset in zip(datasets.keys(), [training_dataset, validation_dataset, test_dataset]):\n",
        "  print(f\"The PyTorch dataset contains {len(dataset)} samples, {'as expected' if len(dataset) == len(datasets[dataset_type]) else 'unexpectedly'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEbkSPO5cH3y"
      },
      "source": [
        "## Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7dx6ekm_Tg4"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "training_loader = DataLoader(dataset=training_dataset,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=True,\n",
        "                             num_workers=2,\n",
        "                             pin_memory=True)\n",
        "validation_loader = DataLoader(dataset=validation_dataset,\n",
        "                               batch_size=BATCH_SIZE,\n",
        "                               shuffle=True,\n",
        "                               num_workers=2,\n",
        "                               pin_memory=True)\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         shuffle=True,\n",
        "                         num_workers=2,\n",
        "                         pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SSt2j56ubAS"
      },
      "source": [
        "## Enter: WaveNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAWj5Bh7sjqb"
      },
      "outputs": [],
      "source": [
        "class CausalConv1d(nn.Conv1d):\n",
        "    \"\"\"1D Causal convolution layer that pads inputs to avoid using future data.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, **kwargs):\n",
        "        padding = (kernel_size - 1) * dilation\n",
        "        super().__init__(in_channels, out_channels, kernel_size,\n",
        "                         padding=padding, dilation=dilation, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = super().forward(x)\n",
        "        if self.padding[0] > 0:\n",
        "            out = out[:, :, :-self.padding[0]]\n",
        "        return out\n",
        "\n",
        "class WaveNetForecaster(nn.Module):\n",
        "    def __init__(self, \n",
        "                 channels_to_forecast=14,\n",
        "                 residual_channels=32,\n",
        "                 skip_channels=64,\n",
        "                 kernel_size=2,\n",
        "                 num_layers=8):\n",
        "        \"\"\" WaveNet-based forecaster model.\n",
        "        Args:\n",
        "            in_channels: Number of input channels.\n",
        "            residual_channels: Number of channels in the residual layers.\n",
        "            skip_channels: Number of channels in the skip connections.\n",
        "            kernel_size: Size of the convolutional kernel.\n",
        "            num_layers: Number of dilated causal convolution layers.\n",
        "        \"\"\"\n",
        "        super(WaveNetForecaster, self).__init__()\n",
        "        self.residual_channels = residual_channels\n",
        "        self.skip_channels = skip_channels\n",
        "\n",
        "        self.input_conv = nn.Conv1d(in_channels=channels_to_forecast,\n",
        "                                    out_channels=residual_channels,\n",
        "                                    kernel_size=1)\n",
        "\n",
        "        # lists to hold the layers for each dilated block\n",
        "        self.filter_convs = nn.ModuleList()\n",
        "        self.gate_convs = nn.ModuleList()\n",
        "        self.residual_convs = nn.ModuleList()\n",
        "        self.skip_convs = nn.ModuleList()\n",
        "\n",
        "        for layer_number in range(num_layers):\n",
        "            \"\"\"Exponentially increasing dilation rate\"\"\"\n",
        "            dilation = 2 ** layer_number\n",
        "            self.filter_convs.append(CausalConv1d(in_channels=residual_channels,\n",
        "                                                  out_channels=residual_channels,\n",
        "                                                  kernel_size=kernel_size,\n",
        "                                                  dilation=dilation))\n",
        "            self.gate_convs.append(CausalConv1d(in_channels=residual_channels,\n",
        "                                                out_channels=residual_channels,\n",
        "                                                kernel_size=kernel_size,\n",
        "                                                dilation=dilation))\n",
        "\n",
        "            self.residual_convs.append(nn.Conv1d(in_channels=residual_channels,\n",
        "                                                 out_channels=residual_channels,\n",
        "                                                 kernel_size=1))\n",
        "            self.skip_convs.append(nn.Conv1d(in_channels=residual_channels,\n",
        "                                             out_channels=skip_channels,\n",
        "                                             kernel_size=1))\n",
        "\n",
        "        self.output_conv1 = nn.Conv1d(in_channels=skip_channels,\n",
        "                                      out_channels=skip_channels,\n",
        "                                      kernel_size=1)\n",
        "        self.output_conv2 = nn.Conv1d(in_channels=skip_channels,\n",
        "                                      out_channels=channels_to_forecast,\n",
        "                                      kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the WaveNet model.\n",
        "        Args:\n",
        "            x: Input tensor of shape [batch, in_channels, input_length].\n",
        "        Returns:\n",
        "            Output tensor of shape [batch, in_channels, input_length].\n",
        "        \"\"\"\n",
        "        x = self.input_conv(x)\n",
        "        skip_sum = None\n",
        "\n",
        "        for filter_conv, gate_conv, res_conv, skip_conv in zip(self.filter_convs, self.gate_convs, self.residual_convs, self.skip_convs):\n",
        "\n",
        "            filt = torch.tanh(filter_conv(x))\n",
        "            gate = torch.sigmoid(gate_conv(x))\n",
        "            out = filt * gate\n",
        "\n",
        "            skip_out = skip_conv(out)\n",
        "            skip_sum = skip_out if skip_sum is None else (skip_sum + skip_out)\n",
        "\n",
        "            x = res_conv(out) + x\n",
        "\n",
        "        out = torch.relu(skip_sum)\n",
        "        out = torch.relu(self.output_conv1(out))\n",
        "        out = self.output_conv2(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLLkhiz1cZVo"
      },
      "source": [
        "### Define functions for training, validation and hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq-0wNLnY0O9"
      },
      "outputs": [],
      "source": [
        "def combine(hyperparameters):\n",
        "  \"\"\"Constructs combinations of hyperparameter values.\n",
        "\n",
        "  Parameters:\n",
        "  hyperparameters -- map between hyperparameter names and candidate values (dict[list])\n",
        "\n",
        "  Returns:\n",
        "  candidates -- combinations of hyperparameters values (list[namedtuple])\n",
        "  \"\"\"\n",
        "\n",
        "  candidate = namedtuple(\"combination\", hyperparameters.keys())\n",
        "  candidates = []\n",
        "  for combination in product(*hyperparameters.values()):\n",
        "    candidates.append(candidate(*combination))\n",
        "  return candidates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_validate(model, device, combination, epochs, dataloaders):\n",
        "  \"\"\"Performs model training and validation.\n",
        "\n",
        "  Parameters:\n",
        "  model -- a PyTorch model instance\n",
        "  device -- where to run computations (torch device object)\n",
        "  combination -- a combination of hyperparameter values (namedtuple)\n",
        "  epochs -- number of model runs (int)\n",
        "  dataloaders -- PyTorch dataloader instances (tuple)\n",
        "  \"\"\"\n",
        "\n",
        "  mse_loss = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(),\n",
        "                         lr=combination.learning_rate,\n",
        "                         weight_decay=combination.weight_decay)\n",
        "  training_loss_log = []\n",
        "  validation_loss_log = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    training_loss = []\n",
        "    model.train()\n",
        "\n",
        "    for batch in dataloaders[0]:\n",
        "      past = batch[0].to(device)\n",
        "      true_future = batch[1].to(device)\n",
        "      softmaxed_true_future = nn.functional.softmax(true_future,\n",
        "                                                    dim=1,\n",
        "                                                    dtype=torch.float32)\n",
        "      generated_future = model(past)\n",
        "      loss = mse_loss(generated_future, softmaxed_true_future)\n",
        "      model.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      loss = loss.detach().cpu().numpy()\n",
        "      training_loss.append(loss)\n",
        "    validation_loss = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for batch in dataloaders[1]:\n",
        "        past = batch[0].to(device)\n",
        "        true_future = batch[1].to(device)\n",
        "        softmaxed_true_future = nn.functional.softmax(true_future,\n",
        "                                                      dim=1,\n",
        "                                                      dtype=torch.float32)\n",
        "        generated_future = model(past)\n",
        "        loss = mse_loss(generated_future, softmaxed_true_future)\n",
        "        loss = loss.detach().cpu().numpy()\n",
        "        validation_loss.append(loss)\n",
        "    training_loss = np.mean(training_loss)\n",
        "    training_loss_log.append(training_loss)\n",
        "    validation_loss = np.mean(validation_loss)\n",
        "    validation_loss_log.append(validation_loss)\n",
        "    print(f\"EPOCH {epoch+1} - TRAINING LOSS: {training_loss: .2f} - VALIDATION LOSS: {validation_loss: .2f}\")\n",
        "    if epoch == epochs-1:\n",
        "      print(\"Finished\")\n",
        "  torch.save(model.state_dict(), 'model_parameters.torch')\n",
        "  return training_loss_log, validation_loss_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-QeJZDOckAe"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_tuning(combinations, device, dataloaders):\n",
        "  \"\"\"Chooses the best combination of hyperparameters.\n",
        "\n",
        "  Parameters:\n",
        "  combinations -- hyperparameter combinations to evaluate (namedtuple)\n",
        "  device -- where to run computations (torch device object)\n",
        "  dataloaders -- PyTorch dataloader instances (tuple)\n",
        "  \"\"\"\n",
        "\n",
        "  scores = []\n",
        "  for combination in combinations:\n",
        "    model=WaveNetForecaster()\n",
        "    model.to(device)\n",
        "    print(f\"Combination {combinations.index(combination)+1} of {len(combinations)}\")\n",
        "    score = train_and_validate(model=model,\n",
        "                               device=device,\n",
        "                               combination=combination,\n",
        "                               epochs=20,\n",
        "                               dataloaders=dataloaders)\n",
        "    scores.append(score)\n",
        "  print(\"Model selection finished!\")\n",
        "  training_scores = []\n",
        "  validation_scores = []\n",
        "  for score in scores:\n",
        "    training, validation = score\n",
        "    training_scores.append(training)\n",
        "    validation_scores.append(validation)\n",
        "  least_validation_score = min(validation_scores)\n",
        "  idx = validation_scores.index(least_validation_score)\n",
        "  winner = combinations[idx]\n",
        "  return winner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipYIspH6coCK"
      },
      "source": [
        "## Perform hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMNqfF83HhvW"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available else torch.device(\"CPU\")\n",
        "print(f\"Device is: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRA3LmxwZlHW"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available else torch.device(\"CPU\")\n",
        "print(f\"Device is: {device}\")\n",
        "print(f\" \")\n",
        "hyperparameter_values_to_try = dict(learning_rate=[1e-5,1e-4,1e-3,1e-2],\n",
        "                                    weight_decay=[1e-5,1e-4,1e-3])\n",
        "\n",
        "hyperparameter_combinations = combine(hyperparameter_values_to_try)\n",
        "\n",
        "print(f\"We will try the following {len(hyperparameter_combinations)} hyperparameter combinations:\")\n",
        "print(\" \")\n",
        "for index, combination in enumerate(hyperparameter_combinations):\n",
        "  print(f\"{index+1}:\", combination)\n",
        "\n",
        "optimal_hyperparameters = hyperparameter_tuning(model=WaveNetForecaster(),\n",
        "                                                combinations=hyperparameter_combinations,\n",
        "                                                device=device,\n",
        "                                                dataloaders=(training_loader, training_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ-5GZO0cqcA"
      },
      "source": [
        "## Train (?) using the best hyperparameters set  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIXWaFN6Hoob"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "model = WaveNetForecaster()\n",
        "model.to(device)\n",
        "\n",
        "outputs = train_and_validate(model=model,\n",
        "                             device=device,\n",
        "                             combination=optimal_hyperparameters,\n",
        "                             epochs=100,\n",
        "                             dataloaders=(training_loader, validation_loader))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
